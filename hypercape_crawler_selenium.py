"""
Hypercape ë¸Œëœë“œ í¬ë¡¤ëŸ¬ (Selenium ë²„ì „)
ë¸Œëœë“œ í˜ì´ì§€ URLì„ ì…ë ¥ë°›ì•„ ë¸Œëœë“œ ì •ë³´ì™€ ëª¨ë“  ìƒí’ˆ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬ Excelë¡œ ì €ì¥
JavaScript ë Œë”ë§ í˜ì´ì§€ ì§€ì›
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import json
from datetime import datetime
from urllib.parse import urljoin
import hypercape_config as config


class HypercapeSeleniumCrawler:
    """Hypercape ë¸Œëœë“œ í¬ë¡¤ëŸ¬ (Selenium ë²„ì „)"""
    
    def __init__(self, headless=True):
        """
        Args:
            headless: Trueë©´ ë¸Œë¼ìš°ì € ì°½ì„ ìˆ¨ê¹€, Falseë©´ ë¸Œë¼ìš°ì € ì°½ í‘œì‹œ
        """
        self.headless = headless
        self.driver = None
        
    def _init_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        print("ğŸŒ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì¤‘...")
        
        chrome_options = Options()
        if self.headless:
            chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument(f'user-agent={config.HEADERS["User-Agent"]}')
        
        # ìë™ìœ¼ë¡œ ChromeDriver ë‹¤ìš´ë¡œë“œ ë° ì„¤ì •
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.wait = WebDriverWait(self.driver, 10)
        
        print("âœ… ë¸Œë¼ìš°ì € ì¤€ë¹„ ì™„ë£Œ")
        
    def _close_driver(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("ğŸ”š ë¸Œë¼ìš°ì € ì¢…ë£Œ")
            
    def _wait_and_get_page_source(self, url, wait_seconds=3):
        """í˜ì´ì§€ ë¡œë“œ í›„ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        self.driver.get(url)
        time.sleep(wait_seconds)  # JavaScript ì‹¤í–‰ ëŒ€ê¸°
        return self.driver.page_source
        
    def extract_brand_info(self, brand_url):
        """ë¸Œëœë“œ ì •ë³´ ì¶”ì¶œ"""
        print(f"\nğŸ“‹ ë¸Œëœë“œ ì •ë³´ ìˆ˜ì§‘ ì¤‘: {brand_url}")
        
        page_source = self._wait_and_get_page_source(brand_url)
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # ë¸Œëœë“œ ID ì¶”ì¶œ
        brand_id = brand_url.rstrip('/').split('/')[-1]
        
        # ë¸Œëœë“œëª…ì€ ë‚˜ì¤‘ì— ìƒí’ˆì—ì„œ ì¶”ì¶œ
        brand_name = f"Brand_{brand_id}"
        
        # ë¸Œëœë“œ ì„¤ëª… ì¶”ì¶œ
        description = ""
        for elem in soup.find_all(string=re.compile(r'As a .* brand', re.I)):
            text = elem.strip()
            if len(text) > 20:
                description = text
                break
        
        # ë¸Œëœë“œ ì´ë¯¸ì§€ URL ì¶”ì¶œ
        brand_image_url = ""
        images = soup.find_all('img')
        for img in images:
            src = img.get('src', '')
            if src and ('brand' in src.lower() or 'logo' in src.lower()):
                brand_image_url = src
                if not brand_image_url.startswith('http'):
                    brand_image_url = urljoin(config.BASE_URL, brand_image_url)
                break
        
        if not brand_image_url:
            for img in images:
                src = img.get('src', '')
                if src and 'icon' not in src.lower() and 'favicon' not in src.lower():
                    brand_image_url = src
                    if not brand_image_url.startswith('http'):
                        brand_image_url = urljoin(config.BASE_URL, brand_image_url)
                    break
        
        brand_data = {
            'brand_id': brand_id,
            'brand_name': brand_name,
            'brand_description': description,
            'brand_image_url': brand_image_url,
            'product_list_url': '',  # ìƒí’ˆ ëª©ë¡ URL ì¶”ê°€
            'total_products': 0,
            'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # "show products" ë§í¬ ì°¾ê¸°
        try:
            show_products = soup.find('a', href=re.compile(r'goods\?brand=', re.I))
            if show_products:
                href = show_products.get('href')
                if href:
                    brand_data['product_list_url'] = urljoin(config.BASE_URL, href)
                    print(f"  â†’ ìƒí’ˆ ëª©ë¡ URL ë°œê²¬: {brand_data['product_list_url']}")
        except Exception as e:
            print(f"  âš ï¸ ìƒí’ˆ ëª©ë¡ URL ì°¾ê¸° ì‹¤íŒ¨: {str(e)}")
            
        # ëª» ì°¾ì•˜ìœ¼ë©´ ê¸°ë³¸ê°’ (í•˜ì§€ë§Œ ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
        if not brand_data['product_list_url']:
             # brand_idê°€ ì•„ë‹ˆë¼ brand_nameì„ ì‚¬ìš©í•´ì•¼ í•¨ (í•˜ì§€ë§Œ brand_nameì„ ì•„ì§ ëª¨ë¥¼ ìˆ˜ ìˆìŒ)
             # ì¼ë‹¨ brand_idë¡œ ì‹œë„í•˜ë˜ ê²½ê³  ì¶œë ¥
             brand_data['product_list_url'] = f"{config.BASE_URL}/goods?brand={brand_id}"
             print(f"  âš ï¸ ìƒí’ˆ ëª©ë¡ URLì„ ì°¾ì§€ ëª»í•´ ê¸°ë³¸ê°’ ì‚¬ìš©: {brand_data['product_list_url']}")
        
        print(f"âœ… ë¸Œëœë“œ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ (ìƒí’ˆì—ì„œ ì •í™•í•œ ë¸Œëœë“œëª… ì¶”ì¶œ ì˜ˆì •)")
        return brand_data
        
    def get_product_links(self, product_list_url):
        """ìƒí’ˆ ëª©ë¡ í˜ì´ì§€ì—ì„œ ìƒí’ˆ ë§í¬ ìˆ˜ì§‘"""
        print(f"\nğŸ” ìƒí’ˆ ëª©ë¡ ìˆ˜ì§‘ ì¤‘: {product_list_url}")
        
        page_source = self._wait_and_get_page_source(product_list_url, wait_seconds=5)
        soup = BeautifulSoup(page_source, 'html.parser')
        
        product_links = []
        for link in soup.find_all('a', href=re.compile(r'/goods/\d+')):
            href = link.get('href')
            full_url = urljoin(config.BASE_URL, href)
            if full_url not in product_links:
                product_links.append(full_url)
        
        print(f"âœ… ìƒí’ˆ {len(product_links)}ê°œ ë°œê²¬")
        return product_links
        
    def extract_product_details(self, product_url):
        """ìƒí’ˆ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        print(f"  ğŸ“¦ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì¤‘: {product_url}")
        
        page_source = self._wait_and_get_page_source(product_url, wait_seconds=3)
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # ìƒí’ˆ ID
        product_id = product_url.rstrip('/').split('/')[-1]
        
        # ìƒí’ˆëª… ì¶”ì¶œ (ìˆ˜ì •ë¨)
        product_name = "Unknown"
        name_elem = soup.select_one('h4#name')
        if not name_elem:
            name_elem = soup.select_one('h4.pro-desc')
        if not name_elem:
            name_elem = soup.select_one('h1')
            
        if name_elem:
            product_name = name_elem.get_text(strip=True)
        
        # ê°€ê²© ì •ë³´ ì¶”ì¶œ (ìˆ˜ì •ë¨)
        price = ""
        original_price = ""
        
        # í˜„ì¬ ê°€ê²©
        price_elem = soup.select_one('span#price')
        if price_elem:
            price = price_elem.get_text(strip=True)
            
        # ì •ê°€ (í• ì¸ ì „ ê°€ê²©)
        org_price_elem = soup.select_one('span#compareAtPrice')
        if org_price_elem:
            original_price = org_price_elem.get_text(strip=True)
            
        # ë§Œì•½ ìœ„ ì„ íƒìë¡œ ëª» ì°¾ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‹œë„
        if not price:
            price_elem = soup.find('span', class_=re.compile(r'price', re.I))
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                prices = re.findall(r'\$[\d.]+', price_text)
                if len(prices) >= 1:
                    price = prices[0]
        
        # í• ì¸ìœ¨
        discount_rate = ""
        if price and original_price:
            try:
                p = float(price.replace('$', '').replace(',', ''))
                op = float(original_price.replace('$', '').replace(',', ''))
                if op > 0:
                    discount_rate = f"{int((1 - p/op) * 100)}%"
            except:
                pass
        
        # ì˜µì…˜
        options = []
        option_section = soup.find('div', string=re.compile(r'Option', re.I))
        if option_section:
            option_parent = option_section.find_parent()
            if option_parent:
                option_items = option_parent.find_all(['button', 'div', 'span'])
                for item in option_items:
                    text = item.get_text(strip=True)
                    if text and text != 'Option':
                        options.append(text)
        
        options_str = ", ".join(options) if options else ""
        
        # ìƒí’ˆ ì„¤ëª…, ì‚¬ìš©ë²•, ì„±ë¶„ ë¶„ë¦¬ ì¶”ì¶œ
        description = ""
        how_to_use = ""
        ingredients = ""
        
        desc_elem = soup.select_one('div#description')
        if desc_elem:
            current_section = "description" # ê¸°ë³¸ ì„¹ì…˜
            
            # ìì‹ ìš”ì†Œë“¤ì„ ìˆœíšŒí•˜ë©° ì²˜ë¦¬
            for child in desc_elem.children:
                if child.name in ['h2', 'h4']:
                    header_text = child.get_text(strip=True).lower()
                    if 'ingredients' in header_text:
                        current_section = "ingredients"
                    elif 'how to use' in header_text:
                        current_section = "how_to_use"
                    elif 'details' in header_text:
                        current_section = "description"
                    else:
                        # [important] ê°™ì€ ê¸°íƒ€ í—¤ë”ëŠ” ì„¤ëª…ì— í¬í•¨
                        if current_section == "description":
                            description += f"\n\n[{child.get_text(strip=True)}]"
                            
                elif child.name == 'pre':
                    text = child.get_text(strip=True)
                    if current_section == "ingredients":
                        ingredients += text + "\n"
                    elif current_section == "how_to_use":
                        how_to_use += text + "\n"
                    else:
                        description += text + "\n"
                        
            # ì•ë’¤ ê³µë°± ì œê±°
            description = description.strip()
            how_to_use = how_to_use.strip()
            ingredients = ingredients.strip()
        
        # ë§Œì•½ ìœ„ ë°©ì‹ìœ¼ë¡œ ì¶”ì¶œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ê¸°ì¡´ ë°©ì‹ ì‹œë„ (ë°±ì—…)
        if not description and not how_to_use and not ingredients:
            desc_section = soup.find('div', string=re.compile(r'Description', re.I))
            if desc_section:
                desc_parent = desc_section.find_next_sibling()
                if desc_parent:
                    description = desc_parent.get_text(strip=True)
        
        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
        main_image_url = ""
        detail_images_urls = []
        
        # ë©”ì¸ ì´ë¯¸ì§€
        main_img = soup.find('img', alt=re.compile(product_name[:20], re.I))
        if not main_img:
            images = soup.find_all('img')
            for img in images:
                src = img.get('src', '')
                if src and 'product' in src.lower():
                    main_img = img
                    break
        
        if main_img:
            main_image_url = main_img.get('src', '')
            if main_image_url and not main_image_url.startswith('http'):
                main_image_url = urljoin(config.BASE_URL, main_image_url)
        
        # ëª¨ë“  ìƒí’ˆ ì´ë¯¸ì§€
        for img in soup.find_all('img'):
            src = img.get('src', '')
            if src and ('product' in src.lower() or 'goods' in src.lower() or 'image' in src.lower()):
                if not src.startswith('http'):
                    src = urljoin(config.BASE_URL, src)
                if src not in detail_images_urls and src != main_image_url:
                    detail_images_urls.append(src)
        
        product_data = {
            'product_id': product_id,
            'product_name': product_name,
            'price': price,
            'original_price': original_price,
            'discount_rate': discount_rate,
            'options': options_str,
            'description': description,
            'how_to_use': how_to_use,
            'ingredients': ingredients,
            'main_image_url': main_image_url,
            'detail_images_urls': json.dumps(detail_images_urls),
            'product_url': product_url
        }
        
        print(f"  âœ… ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {product_name[:50]}")
        return product_data
        
    def save_to_excel(self, brand_data, products_data, output_path):
        """Excel íŒŒì¼ë¡œ ì €ì¥"""
        print(f"\nğŸ’¾ Excel íŒŒì¼ ì €ì¥ ì¤‘: {output_path}")
        
        import os
        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)
        
        brand_df = pd.DataFrame([brand_data])
        products_df = pd.DataFrame(products_data)
        
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            brand_df.to_excel(writer, sheet_name='Brand', index=False)
            products_df.to_excel(writer, sheet_name='Products', index=False)
        
        print(f"âœ… Excel íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        
    def crawl_brand(self, brand_url):
        """ë¸Œëœë“œ ì „ì²´ í¬ë¡¤ë§"""
        print("="*80)
        print("ğŸš€ Hypercape ë¸Œëœë“œ í¬ë¡¤ëŸ¬ ì‹œì‘ (Selenium)")
        print("="*80)
        
        try:
            # ë¸Œë¼ìš°ì € ì´ˆê¸°í™”
            self._init_driver()
            
            # 1. ë¸Œëœë“œ ì •ë³´ ì¶”ì¶œ
            brand_data = self.extract_brand_info(brand_url)
            if not brand_data:
                print("âŒ ë¸Œëœë“œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            brand_name = brand_data['brand_name']
            brand_id = brand_data['brand_id']
            product_list_url = brand_data['product_list_url']
            
            # 2. ìƒí’ˆ ë§í¬ ìˆ˜ì§‘
            product_links = self.get_product_links(product_list_url)
            if not product_links:
                print("âŒ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            brand_data['total_products'] = len(product_links)
            
            # 3. ê° ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘
            products_data = []
            for i, product_url in enumerate(product_links, 1):
                print(f"\n[{i}/{len(product_links)}]")
                product_data = self.extract_product_details(product_url)
                if product_data:
                    # ì²« ë²ˆì§¸ ìƒí’ˆì—ì„œ ë¸Œëœë“œëª… ì¶”ì¶œ
                    if i == 1 and product_data['product_name']:
                        match = re.match(r'\[([^\]]+)\]', product_data['product_name'])
                        if match:
                            actual_brand_name = match.group(1)
                            brand_data['brand_name'] = actual_brand_name
                            brand_name = actual_brand_name
                            print(f"  âœ… ë¸Œëœë“œëª… ì—…ë°ì´íŠ¸: {brand_name}")
                    
                    product_data['brand_name'] = brand_name
                    products_data.append(product_data)
            
            # 4. Excel ì €ì¥
            output_filename = f"{brand_name}_products.xlsx"
            output_path = f"{config.OUTPUT_DIR}/{output_filename}"
            self.save_to_excel(brand_data, products_data, output_path)
            
            print("\n" + "="*80)
            print("âœ¨ í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ğŸ“Š ë¸Œëœë“œ: {brand_name}")
            print(f"ğŸ“¦ ìƒí’ˆ ìˆ˜: {len(products_data)}")
            print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {output_path}")
            print("="*80)
            
            return output_path
            
        finally:
            # ë¸Œë¼ìš°ì € ì¢…ë£Œ
            self._close_driver()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    brand_url = ""
    
    # ëª…ë ¹í–‰ ì¸ìê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    if len(sys.argv) >= 2:
        brand_url = sys.argv[1]
    # ì—†ìœ¼ë©´ ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    else:
        print("í¬ë¡¤ë§í•  ë¸Œëœë“œ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        print("ì˜ˆì‹œ: https://biz.hypercape.com/brands/149")
        try:
            brand_url = input("URL ì…ë ¥: ").strip()
        except KeyboardInterrupt:
            print("\nì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            sys.exit(0)
            
    if not brand_url:
        print("URLì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    # headless=Falseë¡œ ì„¤ì •í•˜ë©´ ë¸Œë¼ìš°ì € ì°½ì´ ë³´ì…ë‹ˆë‹¤ (ë””ë²„ê¹…ìš©)
    crawler = HypercapeSeleniumCrawler(headless=True)
    crawler.crawl_brand(brand_url)


if __name__ == "__main__":
    main()
