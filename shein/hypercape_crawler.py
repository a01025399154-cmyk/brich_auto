"""
Hypercape ë¸Œëœë“œ í¬ë¡¤ëŸ¬
ë¸Œëœë“œ í˜ì´ì§€ URLì„ ì…ë ¥ë°›ì•„ ë¸Œëœë“œ ì •ë³´ì™€ ëª¨ë“  ìƒí’ˆ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬ Excelë¡œ ì €ì¥
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import time
import random
import re
import json
from datetime import datetime
from urllib.parse import urljoin, urlparse
from pathlib import Path
import hypercape_config as config


class HypercapeCrawler:
    """Hypercape ë¸Œëœë“œ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(config.HEADERS)
        
    def _delay(self):
        """ìš”ì²­ ê°„ ëœë¤ ë”œë ˆì´"""
        time.sleep(random.uniform(config.DELAY_MIN, config.DELAY_MAX))
        
    def _get_page(self, url, retries=0):
        """í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ í¬í•¨)"""
        try:
            self._delay()
            response = self.session.get(url, timeout=config.TIMEOUT)
            response.raise_for_status()
            return response
        except requests.RequestException as e:
            if retries < config.MAX_RETRIES:
                print(f"âš ï¸  ìš”ì²­ ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘... ({retries + 1}/{config.MAX_RETRIES}): {url}")
                time.sleep(2 ** retries)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                return self._get_page(url, retries + 1)
            else:
                print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {url} - {str(e)}")
                return None
                
    def extract_brand_info(self, brand_url):
        """ë¸Œëœë“œ ì •ë³´ ì¶”ì¶œ"""
        print(f"\nğŸ“‹ ë¸Œëœë“œ ì •ë³´ ìˆ˜ì§‘ ì¤‘: {brand_url}")
        
        response = self._get_page(brand_url)
        if not response:
            return None
            
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ë¸Œëœë“œ ID ì¶”ì¶œ (URLì—ì„œ)
        brand_id = brand_url.rstrip('/').split('/')[-1]
        
        # ë¸Œëœë“œëª…ì€ ë‚˜ì¤‘ì— ìƒí’ˆì—ì„œ ì¶”ì¶œ (ì¼ë‹¨ ì„ì‹œê°’)
        brand_name = f"Brand_{brand_id}"
        
        # ë¸Œëœë“œ ì„¤ëª… ì¶”ì¶œ ì‹œë„
        description = ""
        for elem in soup.find_all(string=re.compile(r'As a .* brand', re.I)):
            text = elem.strip()
            if len(text) > 20:  # ì¶©ë¶„íˆ ê¸´ ì„¤ëª…
                description = text
                break
        
        # ë¸Œëœë“œ ì´ë¯¸ì§€ URL ì¶”ì¶œ
        brand_image_url = ""
        # í° ì´ë¯¸ì§€ ì°¾ê¸° (ë¸Œëœë“œ ë¡œê³ )
        images = soup.find_all('img')
        for img in images:
            src = img.get('src', '')
            if src and ('brand' in src.lower() or 'logo' in src.lower()):
                brand_image_url = src
                if not brand_image_url.startswith('http'):
                    brand_image_url = urljoin(config.BASE_URL, brand_image_url)
                break
        
        # ì´ë¯¸ì§€ë¥¼ ëª» ì°¾ì•˜ìœ¼ë©´ ì²« ë²ˆì§¸ í° ì´ë¯¸ì§€
        if not brand_image_url:
            for img in images:
                src = img.get('src', '')
                if src and 'icon' not in src.lower() and 'favicon' not in src.lower():
                    brand_image_url = src
                    if not brand_image_url.startswith('http'):
                        brand_image_url = urljoin(config.BASE_URL, brand_image_url)
                    break
        
        brand_data = {
            'brand_id': brand_id,
            'brand_name': brand_name,  # ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸ë¨
            'brand_description': description,
            'brand_image_url': brand_image_url,
            'total_products': 0,
            'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        print(f"âœ… ë¸Œëœë“œ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {brand_name} (ìƒí’ˆì—ì„œ ì •í™•í•œ ì´ë¦„ ì¶”ì¶œ ì˜ˆì •)")
        return brand_data
        
    def get_product_list_url(self, brand_id):
        """ìƒí’ˆ ëª©ë¡ í˜ì´ì§€ URL ê°€ì ¸ì˜¤ê¸°"""
        # ë¸Œëœë“œ IDë¡œ ì§ì ‘ ìƒí’ˆ ëª©ë¡ URL êµ¬ì„±
        # íŒ¨í„´: /goods?brand={brand_id}
        product_list_url = f"{config.BASE_URL}/goods?brand={brand_id}"
        print(f"  â†’ ìƒí’ˆ ëª©ë¡ URL: {product_list_url}")
        return product_list_url
        
    def get_product_links(self, product_list_url):
        """ìƒí’ˆ ëª©ë¡ì—ì„œ ëª¨ë“  ìƒí’ˆ ë§í¬ ìˆ˜ì§‘"""
        print(f"\nğŸ” ìƒí’ˆ ëª©ë¡ ìˆ˜ì§‘ ì¤‘: {product_list_url}")
        
        response = self._get_page(product_list_url)
        if not response:
            return []
            
        soup = BeautifulSoup(response.content, 'html.parser')
        
        product_links = []
        
        # ìƒí’ˆ ë§í¬ ì°¾ê¸° - ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„
        # íŒ¨í„´ 1: /goods/{id} í˜•íƒœì˜ ë§í¬
        for link in soup.find_all('a', href=re.compile(r'/goods/\d+')):
            href = link.get('href')
            full_url = urljoin(config.BASE_URL, href)
            if full_url not in product_links:
                product_links.append(full_url)
        
        print(f"âœ… ìƒí’ˆ {len(product_links)}ê°œ ë°œê²¬")
        return product_links
        
    def extract_product_details(self, product_url):
        """ìƒí’ˆ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        print(f"  ğŸ“¦ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì¤‘: {product_url}")
        
        response = self._get_page(product_url)
        if not response:
            return None
            
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ìƒí’ˆ ID ì¶”ì¶œ
        product_id = product_url.rstrip('/').split('/')[-1]
        
        # ìƒí’ˆëª… ì¶”ì¶œ
        product_name = soup.find('h1')
        if not product_name:
            product_name = soup.find('div', class_=re.compile(r'product.*name', re.I))
        product_name = product_name.get_text(strip=True) if product_name else "Unknown"
        
        # ê°€ê²© ì •ë³´ ì¶”ì¶œ
        price = ""
        original_price = ""
        
        # ê°€ê²© ì°¾ê¸°
        price_elem = soup.find('span', class_=re.compile(r'price', re.I))
        if not price_elem:
            price_elem = soup.find('div', class_=re.compile(r'price', re.I))
        
        if price_elem:
            price_text = price_elem.get_text(strip=True)
            # $14.00 $28.00 í˜•íƒœì—ì„œ ì¶”ì¶œ
            prices = re.findall(r'\$[\d.]+', price_text)
            if len(prices) >= 2:
                price = prices[0]
                original_price = prices[1]
            elif len(prices) == 1:
                price = prices[0]
        
        # í• ì¸ìœ¨ ê³„ì‚°
        discount_rate = ""
        if price and original_price:
            try:
                p = float(price.replace('$', ''))
                op = float(original_price.replace('$', ''))
                if op > 0:
                    discount_rate = f"{int((1 - p/op) * 100)}%"
            except:
                pass
        
        # ì˜µì…˜ ì •ë³´ ì¶”ì¶œ
        options = []
        option_section = soup.find('div', string=re.compile(r'Option', re.I))
        if option_section:
            option_parent = option_section.find_parent()
            if option_parent:
                option_items = option_parent.find_all(['button', 'div', 'span'])
                for item in option_items:
                    text = item.get_text(strip=True)
                    if text and text != 'Option':
                        options.append(text)
        
        options_str = ", ".join(options) if options else ""
        
        # ìƒí’ˆ ì„¤ëª… ì¶”ì¶œ
        description = ""
        desc_section = soup.find('div', string=re.compile(r'Description', re.I))
        if desc_section:
            desc_parent = desc_section.find_next_sibling()
            if desc_parent:
                description = desc_parent.get_text(strip=True)
        
        # Details ì„¹ì…˜ë„ í™•ì¸
        if not description:
            details = soup.find('div', class_=re.compile(r'details', re.I))
            if details:
                description = details.get_text(strip=True)
        
        # ì‚¬ìš©ë²• ì¶”ì¶œ
        how_to_use = ""
        how_section = soup.find('div', string=re.compile(r'How to use', re.I))
        if how_section:
            how_parent = how_section.find_next_sibling()
            if how_parent:
                how_to_use = how_parent.get_text(strip=True)
        
        # ì„±ë¶„ ì •ë³´ ì¶”ì¶œ
        ingredients = ""
        ing_section = soup.find('div', string=re.compile(r'Ingredients', re.I))
        if ing_section:
            ing_parent = ing_section.find_next_sibling()
            if ing_parent:
                ingredients = ing_parent.get_text(strip=True)
        
        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
        main_image_url = ""
        detail_images_urls = []
        
        # ë©”ì¸ ì´ë¯¸ì§€
        main_img = soup.find('img', alt=re.compile(product_name[:20], re.I))
        if not main_img:
            # í° ì´ë¯¸ì§€ ì°¾ê¸°
            images = soup.find_all('img')
            for img in images:
                src = img.get('src', '')
                if src and 'product' in src.lower():
                    main_img = img
                    break
        
        if main_img:
            main_image_url = main_img.get('src', '')
            if main_image_url and not main_image_url.startswith('http'):
                main_image_url = urljoin(config.BASE_URL, main_image_url)
        
        # ëª¨ë“  ìƒí’ˆ ì´ë¯¸ì§€ ìˆ˜ì§‘
        for img in soup.find_all('img'):
            src = img.get('src', '')
            if src and ('product' in src.lower() or 'goods' in src.lower() or 'image' in src.lower()):
                if not src.startswith('http'):
                    src = urljoin(config.BASE_URL, src)
                if src not in detail_images_urls and src != main_image_url:
                    detail_images_urls.append(src)
        
        product_data = {
            'product_id': product_id,
            'product_name': product_name,
            'price': price,
            'original_price': original_price,
            'discount_rate': discount_rate,
            'options': options_str,
            'description': description,
            'how_to_use': how_to_use,
            'ingredients': ingredients,
            'main_image_url': main_image_url,
            'detail_images_urls': json.dumps(detail_images_urls),
            'product_url': product_url
        }
        
        print(f"  âœ… ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {product_name[:50]}")
        return product_data
        

        
    def save_to_excel(self, brand_data, products_data, output_path):
        """Excel íŒŒì¼ë¡œ ì €ì¥"""
        print(f"\nğŸ’¾ Excel íŒŒì¼ ì €ì¥ ì¤‘: {output_path}")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)
        
        # DataFrame ìƒì„±
        brand_df = pd.DataFrame([brand_data])
        products_df = pd.DataFrame(products_data)
        
        # Excel ì €ì¥
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            brand_df.to_excel(writer, sheet_name='Brand', index=False)
            products_df.to_excel(writer, sheet_name='Products', index=False)
        
        print(f"âœ… Excel íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        
    def crawl_brand(self, brand_url):
        """ë¸Œëœë“œ ì „ì²´ í¬ë¡¤ë§"""
        print("="*80)
        print("ğŸš€ Hypercape ë¸Œëœë“œ í¬ë¡¤ëŸ¬ ì‹œì‘")
        print("="*80)
        
        # 1. ë¸Œëœë“œ ì •ë³´ ì¶”ì¶œ
        brand_data = self.extract_brand_info(brand_url)
        if not brand_data:
            print("âŒ ë¸Œëœë“œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        brand_name = brand_data['brand_name']
        brand_id = brand_data['brand_id']
        
        # 2. ìƒí’ˆ ëª©ë¡ URL ê°€ì ¸ì˜¤ê¸°
        product_list_url = self.get_product_list_url(brand_id)
        if not product_list_url:
            print("âŒ ìƒí’ˆ ëª©ë¡ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 3. ìƒí’ˆ ë§í¬ ìˆ˜ì§‘
        product_links = self.get_product_links(product_list_url)
        if not product_links:
            print("âŒ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        brand_data['total_products'] = len(product_links)
        
        # 4. ê° ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘
        products_data = []
        for i, product_url in enumerate(product_links, 1):
            print(f"\n[{i}/{len(product_links)}]")
            product_data = self.extract_product_details(product_url)
            if product_data:
                # ì²« ë²ˆì§¸ ìƒí’ˆì—ì„œ ë¸Œëœë“œëª… ì¶”ì¶œ
                if i == 1 and product_data['product_name']:
                    # "[BIOHEAL BOH] Product Name" í˜•íƒœì—ì„œ ë¸Œëœë“œëª… ì¶”ì¶œ
                    match = re.match(r'\[([^\]]+)\]', product_data['product_name'])
                    if match:
                        actual_brand_name = match.group(1)
                        brand_data['brand_name'] = actual_brand_name
                        brand_name = actual_brand_name
                        print(f"  âœ… ë¸Œëœë“œëª… ì—…ë°ì´íŠ¸: {brand_name}")
                
                product_data['brand_name'] = brand_name
                products_data.append(product_data)
        
        # 5. Excel ì €ì¥
        output_filename = f"{brand_name}_products.xlsx"
        output_path = os.path.join(config.OUTPUT_DIR, output_filename)
        self.save_to_excel(brand_data, products_data, output_path)
        
        print("\n" + "="*80)
        print("âœ¨ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“Š ë¸Œëœë“œ: {brand_name}")
        print(f"ğŸ“¦ ìƒí’ˆ ìˆ˜: {len(products_data)}")
        print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {output_path}")
        print("="*80)
        
        return output_path


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python hypercape_crawler.py <ë¸Œëœë“œ_URL>")
        print("ì˜ˆì‹œ: python hypercape_crawler.py https://biz.hypercape.com/brands/149")
        sys.exit(1)
    
    brand_url = sys.argv[1]
    
    crawler = HypercapeCrawler()
    crawler.crawl_brand(brand_url)


if __name__ == "__main__":
    main()
